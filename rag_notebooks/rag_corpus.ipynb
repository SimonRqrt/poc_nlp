{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"DEPLOYMENT_NAME_LLM\"),\n",
    "    openai_api_version=\"2023-06-01-preview\",\n",
    "    model_version=\"0301\",\n",
    ")\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"DEPLOYMENT_NAME_EMBEDDING\"),\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_client = pinecone.Pinecone(\n",
    "    api_key=pinecone_api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from docx import Document\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(doc_path):\n",
    "    doc = Document(doc_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            documents.append(extract_text_from_pdf(file_path))\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            documents.append(extract_text_from_docx(file_path))\n",
    "    return documents\n",
    "\n",
    "directory_path = \"Documents/Corpus\"\n",
    "corpus = load_documents_from_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of document splits: 348\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "doc_splits = []\n",
    "\n",
    "for text in corpus:\n",
    "    texts = text_splitter.split_text(text)\n",
    "    docs = [Document(page_content=t) for t in texts]\n",
    "    doc_splits.extend(text_splitter.split_documents(docs))\n",
    "\n",
    "print(f\"Total number of document splits: {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"corpus\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "retriever = pinecone.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'hirondelle prévient les petits oiseaux des dangers que représente la culture du chanvre, expliquant qu'ils seront pris au piège et attrapés par les hommes s'ils ne font rien. Les oiseaux ne la croient pas et se moquent d'elle. Plus tard, ils se rendent compte que l'hirondelle avait raison et décident de suivre ses conseils pour éviter les pièges des hommes.\n"
     ]
    }
   ],
   "source": [
    "response_1 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Que raconte l'hirondelle et les petits oiseaux ?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"Le S\"}},\n",
    ")\n",
    "\n",
    "print(response_1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans cette fable, le passager d'un navire en détresse fait un vœu à Jupiter en promettant de lui offrir 100 bœufs s'il est sauvé. Une fois sain et sauf, il brûle quelques os en guise de sacrifice et dit à Jupiter qu'il n'a plus rien à lui devoir. Jupiter feint de rire, mais envoie un songe au passager pour lui dire qu'un trésor l'attend à un certain endroit. Le passager court alors chercher le trésor, mais se fait voler et n'ayant plus qu'un écu, il leur promet cent talents d'or en échange du trésor.\n"
     ]
    }
   ],
   "source": [
    "response_2 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Que raconte Jupiter et le passager ?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"Le S\"}},\n",
    ")\n",
    "\n",
    "print(response_2[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
