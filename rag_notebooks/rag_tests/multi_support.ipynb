{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import chromadb\n",
    "import pymupdf\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WikipediaLoader, YoutubeLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "client = chromadb.Client()\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"DEPLOYMENT_NAME_LLM\"),\n",
    "    openai_api_version=\"2023-06-01-preview\",\n",
    "    model_version=\"0301\",\n",
    ")\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"DEPLOYMENT_NAME_EMBEDDING\"),\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wiki(page_name):\n",
    "    \"\"\"\n",
    "    Récupère les données d'une page wikipedia et les retourne\n",
    "    sous la forme d'une liste de plusieurs documents\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "    )\n",
    "    docs = WikipediaLoader(query=page_name, load_max_docs=1, doc_content_chars_max=10000).load()\n",
    "    doc_splits = text_splitter.split_documents(docs)\n",
    "    return doc_splits\n",
    "\n",
    "def create_vectorstore_and_retriever(docs, embedding, client, username):\n",
    "    \"\"\"\n",
    "    Crée un vectorstore de documents qui est enregistré au sein\n",
    "    d'une collection du client Chroma, et retourne un retriever\n",
    "    de ce vectorstore.\n",
    "    \"\"\"\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        client=client,\n",
    "        collection_name=f\"{username}_collection\",\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retourne l'historique des messages avec un Chat d'une session donnée.\n",
    "    Si il n'existe pas d'historique pour cette session, en crée un.\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def create_prompts_and_chains(llm, retriever):\n",
    "    \"\"\"\n",
    "    Crée et retourne une rag chain où le retriever et le llm ont accès\n",
    "    à l'historique des messages du Chat.\n",
    "    \"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    return conversational_rag_chain\n",
    "\n",
    "def execute_rag_chain(conversational_rag_chain, username, query):\n",
    "    \"\"\"\n",
    "    Transmets une query et une session utilisateur \n",
    "    à une rag chain et retourne la réponse.\n",
    "    \"\"\"\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\"configurable\": {\"session_id\": username}}\n",
    "    )\n",
    "    return response[\"answer\"]\n",
    "\n",
    "def get_answer_llm(username, retriever, query, llm):\n",
    "    \"\"\"Crée une rag chain, lui transmets une query et retourne la réponse.\"\"\"\n",
    "    conversational_rag_chain = create_prompts_and_chains(llm, retriever)\n",
    "    answer = execute_rag_chain(conversational_rag_chain, username, query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(source, username):\n",
    "    global retriever\n",
    "    docs = preprocess_wiki(source)\n",
    "    retriever = create_vectorstore_and_retriever(docs, embedding, client, username)\n",
    "\n",
    "def query_llm(query, username):\n",
    "    return get_answer_llm(username, retriever, query, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"Q\"\n",
    "\n",
    "create_retriever(\"Mistral AI\", username)\n",
    "collection = client.get_collection(f\"{username}_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_retriever(\"Apple Inc.\", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Q_collection' id=UUID('0b61fd91-3d43-413e-8a00-ae9ec4d83a88') metadata=None tenant='default_tenant' database='default_database'\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(collection)\n",
    "print(collection.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
